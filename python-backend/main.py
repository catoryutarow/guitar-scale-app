"""
éŸ³æºè§£æãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ API (FastAPI)

Phase 3 å®Ÿè£…å†…å®¹ï¼š
- POST /analyze ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆï¼ˆãƒ€ãƒŸãƒ¼è§£æãƒ­ã‚¸ãƒƒã‚¯ï¼‰
- Next.js ã‹ã‚‰ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å—ã‘ä»˜ã‘
- ãƒ€ãƒŸãƒ¼ã®è§£æçµæœã‚’è¿”å´

Phase 4 ä»¥é™ã§å®Ÿè£…äºˆå®šï¼š
- å®Ÿéš›ã®stemåˆ†è§£ï¼ˆDemucsï¼‰
- ã‚³ãƒ¼ãƒ‰è§£æï¼ˆlibrosa / madmomï¼‰
- ãƒ†ãƒ³ãƒãƒ»ã‚­ãƒ¼æ¤œå‡º
"""

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from typing import List, Dict, Optional
import os
from pathlib import Path
from dotenv import load_dotenv
import requests
import tempfile

# ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿
load_dotenv()

# ============================================
# FastAPI ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³åˆæœŸåŒ–
# ============================================

app = FastAPI(
    title="Audio Analysis API",
    description="éŸ³æºè§£æãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰API - ã‚³ãƒ¼ãƒ‰é€²è¡Œãƒ»stemåˆ†è§£ãƒ»ã‚¹ã‚±ãƒ¼ãƒ«æ¤œå‡º",
    version="0.1.0 (Phase 3 - Dummy)"
)

# CORSè¨­å®šï¼ˆNext.js ã‹ã‚‰ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’è¨±å¯ï¼‰
ALLOWED_ORIGINS = os.getenv("ALLOWED_ORIGINS", "").split(",") if os.getenv("ALLOWED_ORIGINS") else [
    "http://localhost:3000",  # Next.js é–‹ç™ºã‚µãƒ¼ãƒãƒ¼
    "http://127.0.0.1:3000",
    "https://guitar-scale.com",  # æœ¬ç•ªãƒ‰ãƒ¡ã‚¤ãƒ³
    "https://www.guitar-scale.com",  # wwwä»˜ã
    "https://*.vercel.app",  # Vercel ãƒ‡ãƒ—ãƒ­ã‚¤ï¼ˆãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼‰
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=ALLOWED_ORIGINS,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ============================================
# Pydantic ãƒ¢ãƒ‡ãƒ«å®šç¾©ï¼ˆTypeScript å‹ã¨å¯¾å¿œï¼‰
# ============================================

class AnalysisOptions(BaseModel):
    """è§£æã‚ªãƒ—ã‚·ãƒ§ãƒ³"""
    separateStems: bool = True
    analysisDepth: str = "basic"  # "basic" | "detailed"


class AnalyzeRequest(BaseModel):
    """è§£æãƒªã‚¯ã‚¨ã‚¹ãƒˆ"""
    jobId: str = Field(..., description="ã‚¸ãƒ§ãƒ–ID")
    filePath: str = Field(..., description="éŸ³æºãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹")
    options: Optional[AnalysisOptions] = None


class DetectedKeyInfo(BaseModel):
    """æ¤œå‡ºã•ã‚ŒãŸã‚­ãƒ¼æƒ…å ±"""
    key: str
    scale: str
    confidence: float
    occurrence: float  # å‡ºç¾å‰²åˆï¼ˆ0.0-1.0ï¼‰


class AnalysisMetadata(BaseModel):
    """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿"""
    duration: float
    tempo: float
    timeSignature: str
    detectedKey: str  # ä¸»è¦ã‚­ãƒ¼ï¼ˆæœ€ã‚‚å¤šãå‡ºç¾ï¼‰
    scale: str
    confidence: float
    detectedKeys: Optional[List[DetectedKeyInfo]] = None  # è¤‡æ•°ã‚­ãƒ¼æ¤œå‡ºæ™‚


class ChordInfo(BaseModel):
    """ã‚³ãƒ¼ãƒ‰æƒ…å ±"""
    startTime: float
    endTime: float
    chord: str
    rootNote: str
    quality: str
    confidence: float


class ScaleMatchInfo(BaseModel):
    """ã‚¹ã‚±ãƒ¼ãƒ«ãƒãƒƒãƒãƒ³ã‚°æƒ…å ±"""
    scale: str
    rootNote: str
    matchRate: float
    matchingChords: List[str]


class ScaleMatchResult(BaseModel):
    """ã‚¹ã‚±ãƒ¼ãƒ«ãƒãƒƒãƒãƒ³ã‚°çµæœ"""
    matchingScales: List[ScaleMatchInfo]


class AnalysisResult(BaseModel):
    """è§£æçµæœ"""
    metadata: AnalysisMetadata
    chordProgression: List[ChordInfo]
    scaleMatch: ScaleMatchResult
    stems: Optional[Dict[str, str]] = None


class AnalyzeResponse(BaseModel):
    """è§£æãƒ¬ã‚¹ãƒãƒ³ã‚¹"""
    success: bool
    jobId: str
    status: str  # "completed" | "failed"
    result: Optional[AnalysisResult] = None
    error: Optional[str] = None


# ============================================
# ç’°å¢ƒå¤‰æ•°ã«ã‚ˆã‚‹è§£æãƒ¢ãƒ¼ãƒ‰åˆ‡ã‚Šæ›¿ãˆ
# ============================================

USE_REAL_ANALYSIS = os.getenv("USE_REAL_ANALYSIS", "true").lower() == "true"
USE_ESSENTIA = os.getenv("USE_ESSENTIA", "false").lower() == "true"

print("=" * 60)
print("ğŸµ Audio Analysis API - Startup")
print("=" * 60)
print(f"Analysis Mode: {'REAL' if USE_REAL_ANALYSIS else 'DUMMY (å›ºå®šå€¤)'}")
if USE_REAL_ANALYSIS:
    print(f"  Engine: {'Essentia (é«˜ç²¾åº¦)' if USE_ESSENTIA else 'librosa (æ¨™æº–)'}")
print(f"Environment: USE_REAL_ANALYSIS={os.getenv('USE_REAL_ANALYSIS', 'not set')}")
print(f"Environment: USE_ESSENTIA={os.getenv('USE_ESSENTIA', 'not set')}")

# librosa/essentiaã¯å®Ÿè§£æãƒ¢ãƒ¼ãƒ‰ã§ã®ã¿ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆä¾å­˜é–¢ä¿‚ã‚’æ¸›ã‚‰ã™ãŸã‚ï¼‰
ESSENTIA_AVAILABLE = False

if USE_REAL_ANALYSIS:
    try:
        import librosa
        import numpy as np
        print("âœ“ librosa loaded successfully")
        print("âœ“ numpy loaded successfully")
    except ImportError as e:
        print(f"âš  Warning: librosa not available - {e}")
        print("  Falling back to DUMMY analysis mode")
        USE_REAL_ANALYSIS = False

    # Essentiaã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’è©¦è¡Œ
    if USE_ESSENTIA:
        try:
            import essentia.standard as es
            ESSENTIA_AVAILABLE = True
            print("âœ“ essentia loaded successfully")
        except ImportError as e:
            print(f"âš  Warning: essentia not available - {e}")
            print("  Falling back to librosa analysis")
            ESSENTIA_AVAILABLE = False

    print("=" * 60)
else:
    print("â„¹ Using DUMMY mode (set USE_REAL_ANALYSIS=true for real analysis)")
    print("=" * 60)

# ============================================
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
# ============================================

def download_file_if_url(file_path: str) -> str:
    """
    URLã®å ´åˆã¯ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã€
    ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ã®å ´åˆã¯ãã®ã¾ã¾è¿”ã™

    Args:
        file_path: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã¾ãŸã¯URL

    Returns:
        str: ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    """
    # URLã‹ã©ã†ã‹ã‚’åˆ¤å®š
    if file_path.startswith('http://') or file_path.startswith('https://'):
        print(f"  â†’ Downloading file from URL: {file_path}")

        try:
            # URLã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            response = requests.get(file_path, timeout=30)
            response.raise_for_status()

            # æ‹¡å¼µå­ã‚’å–å¾—ï¼ˆURLã‹ã‚‰ï¼‰
            ext = '.mp3'  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
            if '.' in file_path:
                ext = '.' + file_path.split('.')[-1].split('?')[0]  # ã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’é™¤å»

            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
            with tempfile.NamedTemporaryFile(delete=False, suffix=ext) as tmp_file:
                tmp_file.write(response.content)
                local_path = tmp_file.name

            print(f"  â†’ Downloaded to: {local_path} ({len(response.content)} bytes)")
            return local_path

        except requests.RequestException as e:
            raise HTTPException(
                status_code=400,
                detail=f"Failed to download file from URL: {str(e)}"
            )
    else:
        # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ã®å ´åˆã¯ãã®ã¾ã¾è¿”ã™
        print(f"  â†’ Using local file: {file_path}")
        return file_path


# ============================================
# ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
# ============================================

@app.get("/")
def read_root():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
    return {
        "status": "ok",
        "message": "Audio Analysis API is running",
        "version": "0.1.0 (Phase 3 - Dummy)",
        "endpoints": [
            {"path": "/analyze", "method": "POST", "description": "éŸ³æºè§£æ"}
        ]
    }


@app.post("/analyze", response_model=AnalyzeResponse)
async def analyze_audio(request: AnalyzeRequest):
    """
    éŸ³æºè§£æã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ

    Phase 3 å®Ÿè£…ï¼š
    - ãƒ€ãƒŸãƒ¼ã®è§£æçµæœã‚’è¿”å´
    - ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®å­˜åœ¨ç¢ºèªã®ã¿å®Ÿæ–½

    Phase 4 ä»¥é™ï¼š
    - å®Ÿéš›ã®éŸ³æºãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
    - Demucs ã§ stem åˆ†è§£
    - librosa / madmom ã§ã‚³ãƒ¼ãƒ‰è§£æ
    - ãƒ†ãƒ³ãƒãƒ»ã‚­ãƒ¼æ¤œå‡º
    """

    try:
        print("\n" + "=" * 60)
        print(f"ğŸµ Analyzing audio file")
        print("=" * 60)
        print(f"Job ID: {request.jobId}")
        print(f"File Path/URL: {request.filePath}")
        print(f"Options: {request.options}")
        print(f"Analysis Mode: {'REAL (librosa)' if USE_REAL_ANALYSIS else 'DUMMY (å›ºå®šå€¤)'}")
        print("=" * 60)

        # URLã®å ´åˆã¯ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã€ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ã®å ´åˆã¯ãã®ã¾ã¾ä½¿ç”¨
        local_file_path = download_file_if_url(request.filePath)

        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
        file_path = Path(local_file_path)

        if not file_path.exists():
            raise HTTPException(
                status_code=404,
                detail=f"Audio file not found: {local_file_path}"
            )

        # ç’°å¢ƒå¤‰æ•°ã«å¿œã˜ã¦å®Ÿè§£æ or ãƒ€ãƒŸãƒ¼è§£æã‚’é¸æŠ
        if USE_REAL_ANALYSIS:
            print("â†’ Calling analyze_audio_real()...")
            result = analyze_audio_real(
                file_path=local_file_path,
                options=request.options.dict() if request.options else {}
            )
        else:
            print("â†’ Calling analyze_audio_dummy()...")
            result = analyze_audio_dummy(
                file_path=local_file_path,
                options=request.options.dict() if request.options else {}
            )

        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è¿”å´
        print(f"\nâœ“ Analysis completed for job {request.jobId}")
        print("=" * 60 + "\n")

        return AnalyzeResponse(
            success=True,
            jobId=request.jobId,
            status="completed",
            result=result
        )

    except FileNotFoundError as e:
        print(f"\nâœ— File not found error: {str(e)}")
        print("=" * 60 + "\n")
        raise HTTPException(status_code=404, detail=str(e))

    except Exception as e:
        print(f"\nâœ— Analysis error: {str(e)}")
        import traceback
        traceback.print_exc()
        print("=" * 60 + "\n")
        raise HTTPException(
            status_code=500,
            detail=f"Analysis failed: {str(e)}"
        )
# ============================================
# ãƒ€ãƒŸãƒ¼è§£æãƒ­ã‚¸ãƒƒã‚¯ï¼ˆPhase 3 å®Ÿè£…ï¼‰
# ============================================

def analyze_audio_dummy(file_path: str, options: Dict) -> AnalysisResult:
    """
    ãƒ€ãƒŸãƒ¼è§£æãƒ­ã‚¸ãƒƒã‚¯

    Phase 3: å›ºå®šã®ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™
    Phase 4: ã“ã“ã‚’å®Ÿéš›ã®è§£æãƒ­ã‚¸ãƒƒã‚¯ã«å·®ã—æ›¿ãˆã‚‹

    Args:
        file_path: éŸ³æºãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        options: è§£æã‚ªãƒ—ã‚·ãƒ§ãƒ³

    Returns:
        AnalysisResult: è§£æçµæœ
    """

    print(f"\n[Dummy Analysis] Returning fixed dummy data...")
    print(f"  File: {file_path} (not actually analyzed)")
    print(f"  Result: G ãƒ¡ã‚¸ãƒ£ãƒ¼, 120.0 BPM, 8 chords (å›ºå®šå€¤)")
    print("=" * 60)

    # TODO (Phase 4): å®Ÿéš›ã®éŸ³æºè§£æå‡¦ç†ã«å·®ã—æ›¿ãˆ
    # 1. librosa ã§éŸ³æºã‚’èª­ã¿è¾¼ã¿
    # y, sr = librosa.load(file_path)
    #
    # 2. ãƒ†ãƒ³ãƒæ¤œå‡º
    # tempo, _ = librosa.beat.beat_track(y=y, sr=sr)
    #
    # 3. ã‚­ãƒ¼æ¤œå‡º
    # chroma = librosa.feature.chroma_cqt(y=y, sr=sr)
    # key = estimate_key(chroma)
    #
    # 4. ã‚³ãƒ¼ãƒ‰é€²è¡Œæ¤œå‡º
    # chords = detect_chords(y, sr)
    #
    # 5. stem åˆ†è§£ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    # if options.get('separateStems'):
    #     stems = separate_stems(file_path)

    # Phase 3: ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™
    metadata = AnalysisMetadata(
        duration=120.0,
        tempo=120.0,
        timeSignature="4/4",
        detectedKey="G",
        scale="ãƒ¡ã‚¸ãƒ£ãƒ¼",
        confidence=0.95
    )

    chord_progression = [
        ChordInfo(
            startTime=0.0, endTime=4.0,
            chord="G", rootNote="G", quality="maj", confidence=0.9
        ),
        ChordInfo(
            startTime=4.0, endTime=8.0,
            chord="Em", rootNote="E", quality="min", confidence=0.88
        ),
        ChordInfo(
            startTime=8.0, endTime=12.0,
            chord="C", rootNote="C", quality="maj", confidence=0.87
        ),
        ChordInfo(
            startTime=12.0, endTime=16.0,
            chord="D", rootNote="D", quality="maj", confidence=0.9
        ),
        ChordInfo(
            startTime=16.0, endTime=20.0,
            chord="G", rootNote="G", quality="maj", confidence=0.92
        ),
        ChordInfo(
            startTime=20.0, endTime=24.0,
            chord="Em", rootNote="E", quality="min", confidence=0.85
        ),
        ChordInfo(
            startTime=24.0, endTime=28.0,
            chord="Am", rootNote="A", quality="min", confidence=0.89
        ),
        ChordInfo(
            startTime=28.0, endTime=32.0,
            chord="D", rootNote="D", quality="maj", confidence=0.91
        ),
    ]

    scale_match = ScaleMatchResult(
        matchingScales=[
            ScaleMatchInfo(
                scale="ãƒ¡ã‚¸ãƒ£ãƒ¼",
                rootNote="G",
                matchRate=0.95,
                matchingChords=["G", "C", "D", "Em", "Am"]
            ),
            ScaleMatchInfo(
                scale="ãƒã‚¤ãƒŠãƒ¼",
                rootNote="E",
                matchRate=0.88,
                matchingChords=["Em", "G", "C", "D", "Am"]
            ),
            ScaleMatchInfo(
                scale="ãƒŸã‚¯ã‚½ãƒªãƒ‡ã‚£ã‚¢ãƒ³",
                rootNote="G",
                matchRate=0.82,
                matchingChords=["G", "C", "D", "Em"]
            ),
        ]
    )

    return AnalysisResult(
        metadata=metadata,
        chordProgression=chord_progression,
        scaleMatch=scale_match,
        stems=None  # Phase 4 ã§ stem åˆ†è§£çµæœã‚’è¿”ã™
    )


# ============================================
# å®Ÿè§£æãƒ­ã‚¸ãƒƒã‚¯ï¼ˆPhase 4 - librosa ãƒ™ãƒ¼ã‚¹ï¼‰
# ============================================

def analyze_audio_real(file_path: str, options: Dict) -> AnalysisResult:
    """
    å®Ÿéš›ã®éŸ³æºè§£æãƒ­ã‚¸ãƒƒã‚¯

    Essentiaå„ªå…ˆã€librosaãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼š
    - USE_ESSENTIA=true ã‹ã¤ essentiaåˆ©ç”¨å¯èƒ½ â†’ Essentiaè§£æ
    - ãã‚Œä»¥å¤– â†’ librosaè§£æ

    Args:
        file_path: éŸ³æºãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        options: è§£æã‚ªãƒ—ã‚·ãƒ§ãƒ³

    Returns:
        AnalysisResult: è§£æçµæœ
    """

    # Essentiaå„ªå…ˆ
    if ESSENTIA_AVAILABLE:
        try:
            print(f"\n[Real Analysis] Using Essentia (é«˜ç²¾åº¦ãƒ¢ãƒ¼ãƒ‰)...")
            return analyze_audio_essentia(file_path, options)
        except Exception as e:
            print(f"âš  Essentia analysis failed: {e}")
            print("  Falling back to librosa analysis...")

    # librosaãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    return analyze_audio_librosa(file_path, options)


def analyze_audio_librosa(file_path: str, options: Dict) -> AnalysisResult:
    """
    librosaãƒ™ãƒ¼ã‚¹ã®éŸ³æºè§£æãƒ­ã‚¸ãƒƒã‚¯ï¼ˆé«˜ç²¾åº¦ç‰ˆï¼‰

    æ”¹å–„ç‚¹ï¼š
    - ãƒ©ã‚¦ãƒ‰ãƒã‚¹ãƒ™ãƒ¼ã‚¹ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆé¸æŠï¼ˆæœ€ã‚‚ã‚¨ãƒãƒ«ã‚®ãƒ¼ã®é«˜ã„åŒºé–“ã‚’è§£æï¼‰
    - HPSSï¼ˆHarmonic-Percussive Source Separationï¼‰ã§ãƒãƒ¼ãƒ¢ãƒ‹ãƒƒã‚¯æˆåˆ†ã‚’æŠ½å‡º
    - CQT/CENSãƒ™ãƒ¼ã‚¹ã®Chromaç‰¹å¾´é‡ã§å®‰å®šã—ãŸãƒ”ãƒƒãƒæ¤œå‡º
    - ãƒ“ãƒ¼ãƒˆåŒæœŸã‚³ãƒ¼ãƒ‰æ¤œå‡º

    Args:
        file_path: éŸ³æºãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        options: è§£æã‚ªãƒ—ã‚·ãƒ§ãƒ³

    Returns:
        AnalysisResult: è§£æçµæœ
    """

    print(f"\n[Librosa Analysis] Starting high-precision analysis...")
    print(f"  File: {file_path}")

    # 1. éŸ³å£°èª­ã¿è¾¼ã¿ï¼ˆå…¨ä½“ã‚’èª­ã¿è¾¼ã‚“ã§ã‹ã‚‰æœ€é©ãªåŒºé–“ã‚’é¸æŠï¼‰
    try:
        print(f"  Step 1/6: Loading audio file...")
        y_full, sr = librosa.load(file_path, sr=22050, mono=True)  # 22050Hzã§çµ±ä¸€
        full_duration = len(y_full) / sr
        print(f"    âœ“ Loaded: {full_duration:.2f}s, sr={sr}Hz, samples={len(y_full)}")
    except Exception as e:
        print(f"    âœ— Failed to load audio file: {str(e)}")
        raise Exception(f"Failed to load audio file: {str(e)}")

    # 2. HPSSï¼ˆãƒãƒ¼ãƒ¢ãƒ‹ãƒƒã‚¯ãƒ»ãƒ‘ãƒ¼ã‚«ãƒƒã‚·ãƒ–åˆ†é›¢ï¼‰- å…¨ä½“ã«é©ç”¨
    try:
        print(f"  Step 2/7: Applying Harmonic-Percussive separation (full track)...")
        y_harmonic_full, y_percussive_full = librosa.effects.hpss(y_full)
        print(f"    âœ“ Separated harmonic and percussive components")
    except Exception as e:
        print(f"    âš  HPSS failed: {e}, using original signal")
        y_harmonic_full = y_full

    # 3. ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ†å‰²ã‚­ãƒ¼æ¤œå‡ºï¼ˆè»¢èª¿å¯¾å¿œï¼‰
    try:
        print(f"  Step 3/7: Detecting keys with modulation analysis...")
        detected_keys_info, detected_key, scale_name, confidence = detect_keys_with_modulation(
            y_harmonic_full, sr, segment_duration=15.0
        )

        if len(detected_keys_info) == 1:
            print(f"    âœ“ Single key detected: {detected_key} {scale_name} (confidence: {confidence:.2f})")
        else:
            print(f"    âœ“ Multiple keys detected:")
            for ki in detected_keys_info:
                print(f"      - {ki['key']} {ki['scale']}: {ki['occurrence']*100:.0f}%")
    except Exception as e:
        print(f"    âš  Key detection failed: {e}, using default C major")
        detected_key = "C"
        scale_name = "ãƒ¡ã‚¸ãƒ£ãƒ¼"
        confidence = 0.5
        detected_keys_info = [{"key": "C", "scale": "ãƒ¡ã‚¸ãƒ£ãƒ¼", "confidence": 0.5, "occurrence": 1.0}]

    # 4. ãƒ©ã‚¦ãƒ‰ãƒã‚¹ãƒ™ãƒ¼ã‚¹ã®æœ€é©åŒºé–“é¸æŠï¼ˆã‚³ãƒ¼ãƒ‰æ¤œå‡ºç”¨ï¼‰
    try:
        print(f"  Step 4/7: Selecting optimal segment for chord detection...")
        y, start_time, end_time = select_loudest_segment(y_full, sr, target_duration=30.0)
        y_harmonic, _ = librosa.effects.hpss(y)  # é¸æŠåŒºé–“ã®HPSS
        duration = len(y) / sr
        print(f"    âœ“ Selected segment: {start_time:.1f}s - {end_time:.1f}s ({duration:.1f}s)")
    except Exception as e:
        print(f"    âš  Segment selection failed: {e}, using first 30s")
        y = y_full[:int(30.0 * sr)]
        y_harmonic = y_harmonic_full[:int(30.0 * sr)]
        duration = len(y) / sr
        start_time = 0.0
        end_time = duration

    # 5. ãƒ†ãƒ³ãƒãƒ»ãƒ“ãƒ¼ãƒˆæ¨å®šï¼ˆé¸æŠåŒºé–“ã‹ã‚‰ï¼‰
    try:
        print(f"  Step 5/7: Detecting tempo and beats...")
        tempo, beat_frames = librosa.beat.beat_track(y=y, sr=sr)
        tempo = float(tempo)
        beats = librosa.frames_to_time(beat_frames, sr=sr)
        print(f"    âœ“ Tempo detected: {tempo:.1f} BPM, {len(beats)} beats")
    except Exception as e:
        print(f"    âš  Tempo detection failed: {e}, using default 120 BPM")
        tempo = 120.0
        beats = []

    # 6. ãƒ“ãƒ¼ãƒˆåŒæœŸã‚³ãƒ¼ãƒ‰é€²è¡Œæ¤œå‡ºï¼ˆé¸æŠåŒºé–“ã®ãƒãƒ¼ãƒ¢ãƒ‹ãƒƒã‚¯æˆåˆ†ã‚’ä½¿ç”¨ï¼‰
    try:
        print(f"  Step 6/7: Detecting chord progression (beat-synced)...")
        if len(beats) >= 4:
            chord_progression = detect_chords_beat_synced(
                y_harmonic, sr, beats, detected_key, scale_name, start_time
            )
        else:
            chord_progression = detect_chords_enhanced(
                y_harmonic, sr, detected_key, scale_name, start_time
            )
        print(f"    âœ“ Detected {len(chord_progression)} chord segments")
        if len(chord_progression) > 0:
            print(f"    First chord: {chord_progression[0].chord} ({chord_progression[0].startTime:.1f}s - {chord_progression[0].endTime:.1f}s)")
    except Exception as e:
        print(f"    âš  Chord detection failed: {e}, using fallback")
        chord_progression = generate_fallback_chords(detected_key, full_duration)

    # 7. ã‚¹ã‚±ãƒ¼ãƒ«ãƒãƒƒãƒãƒ³ã‚°ï¼ˆã‚­ãƒ¼ã«åŸºã¥ã„ã¦ï¼‰
    print(f"  Step 7/7: Generating scale matches...")

    try:
        scale_match = generate_scale_match(detected_key, scale_name, chord_progression)
        print(f"    âœ“ Generated {len(scale_match.matchingScales)} scale matches")
    except Exception as e:
        print(f"    âš  Scale matching failed: {e}")
        scale_match = ScaleMatchResult(matchingScales=[])

    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®æ§‹ç¯‰
    # è¤‡æ•°ã‚­ãƒ¼ãŒæ¤œå‡ºã•ã‚ŒãŸå ´åˆã¯detectedKeysã«æ ¼ç´
    detected_keys_list = None
    if len(detected_keys_info) > 1:
        detected_keys_list = [
            DetectedKeyInfo(
                key=ki['key'],
                scale=ki['scale'],
                confidence=ki['confidence'],
                occurrence=ki['occurrence']
            )
            for ki in detected_keys_info
        ]

    metadata = AnalysisMetadata(
        duration=full_duration,
        tempo=tempo,
        timeSignature="4/4",
        detectedKey=detected_key,
        scale=scale_name,
        confidence=confidence,
        detectedKeys=detected_keys_list
    )

    print(f"\n[Librosa Analysis] Analysis completed successfully!")
    print(f"  Result: {detected_key} {scale_name}, {tempo:.1f} BPM, {len(chord_progression)} chords")
    print(f"  Analyzed segment: {start_time:.1f}s - {end_time:.1f}s (loudest {duration:.1f}s)")
    print("=" * 60)

    return AnalysisResult(
        metadata=metadata,
        chordProgression=chord_progression,
        scaleMatch=scale_match,
        stems=None  # Phase 4 ã§ã¯æœªå®Ÿè£…
    )


def detect_keys_with_modulation(y_harmonic, sr, segment_duration=15.0, min_occurrence=0.15):
    """
    ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ†å‰²ã«ã‚ˆã‚‹ã‚­ãƒ¼æ¤œå‡ºï¼ˆè»¢èª¿å¯¾å¿œï¼‰

    æ›²ã‚’ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã«åˆ†å‰²ã—ã€å„ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã§ã‚­ãƒ¼ã‚’æ¨å®šã€‚
    æ¤œå‡ºã•ã‚ŒãŸã‚­ãƒ¼ã‚’é›†è¨ˆã—ã€å‡ºç¾å‰²åˆã¨ã¨ã‚‚ã«è¿”ã™ã€‚

    Args:
        y_harmonic: ãƒãƒ¼ãƒ¢ãƒ‹ãƒƒã‚¯æˆåˆ†
        sr: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆ
        segment_duration: ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®é•·ã•ï¼ˆç§’ï¼‰
        min_occurrence: å ±å‘Šã™ã‚‹æœ€å°å‡ºç¾å‰²åˆï¼ˆã“ã‚Œä»¥ä¸‹ã¯ç„¡è¦–ï¼‰

    Returns:
        Tuple[List[dict], str, str, float]:
            - detected_keys_info: æ¤œå‡ºã•ã‚ŒãŸã‚­ãƒ¼ã®ãƒªã‚¹ãƒˆ
            - primary_key: ä¸»è¦ã‚­ãƒ¼
            - primary_scale: ä¸»è¦ã‚¹ã‚±ãƒ¼ãƒ«
            - confidence: ä¿¡é ¼åº¦
    """
    full_duration = len(y_harmonic) / sr
    num_segments = max(1, int(np.ceil(full_duration / segment_duration)))

    # å„ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã§ã‚­ãƒ¼ã‚’æ¨å®š
    segment_keys = []

    for i in range(num_segments):
        start_time = i * segment_duration
        end_time = min((i + 1) * segment_duration, full_duration)

        start_sample = int(start_time * sr)
        end_sample = int(end_time * sr)
        segment = y_harmonic[start_sample:end_sample]

        # çŸ­ã™ãã‚‹ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã¯ã‚¹ã‚­ãƒƒãƒ—
        if len(segment) < sr * 5:  # 5ç§’æœªæº€
            continue

        try:
            key, scale, conf = estimate_key_enhanced(segment, sr)
            segment_keys.append({
                'key': key,
                'scale': scale,
                'confidence': conf
            })
        except Exception:
            continue

    if not segment_keys:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å…¨ä½“ã‹ã‚‰ã‚­ãƒ¼æ¨å®š
        key, scale, conf = estimate_key_enhanced(y_harmonic, sr)
        return [{"key": key, "scale": scale, "confidence": conf, "occurrence": 1.0}], key, scale, conf

    # ã‚­ãƒ¼+ã‚¹ã‚±ãƒ¼ãƒ«ã®çµ„ã¿åˆã‚ã›ã§é›†è¨ˆ
    key_counts = {}
    for sk in segment_keys:
        key_scale = f"{sk['key']}_{sk['scale']}"
        if key_scale not in key_counts:
            key_counts[key_scale] = {
                'key': sk['key'],
                'scale': sk['scale'],
                'count': 0,
                'confidences': []
            }
        key_counts[key_scale]['count'] += 1
        key_counts[key_scale]['confidences'].append(sk['confidence'])

    # å‡ºç¾å‰²åˆã‚’è¨ˆç®—
    total_segments = len(segment_keys)
    detected_keys_info = []

    for key_scale, data in key_counts.items():
        occurrence = data['count'] / total_segments
        if occurrence >= min_occurrence:
            detected_keys_info.append({
                'key': data['key'],
                'scale': data['scale'],
                'confidence': np.mean(data['confidences']),
                'occurrence': occurrence
            })

    # å‡ºç¾å‰²åˆã§ã‚½ãƒ¼ãƒˆï¼ˆé™é †ï¼‰
    detected_keys_info.sort(key=lambda x: x['occurrence'], reverse=True)

    # ä¸»è¦ã‚­ãƒ¼ï¼ˆæœ€ã‚‚å¤šãå‡ºç¾ï¼‰
    primary = detected_keys_info[0]
    primary_key = primary['key']
    primary_scale = primary['scale']
    primary_confidence = primary['confidence']

    return detected_keys_info, primary_key, primary_scale, primary_confidence


def select_loudest_segment(y, sr, target_duration=30.0, hop_length=512):
    """
    ãƒ©ã‚¦ãƒ‰ãƒã‚¹ãƒ™ãƒ¼ã‚¹ã§æœ€ã‚‚éŸ³é‡ãŒé«˜ã„åŒºé–“ã‚’é¸æŠ

    RMSã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚’è¨ˆç®—ã—ã€æœ€ã‚‚ã‚¨ãƒãƒ«ã‚®ãƒ¼ãŒé›†ä¸­ã—ã¦ã„ã‚‹åŒºé–“ã‚’è¿”ã™ã€‚
    ã“ã‚Œã«ã‚ˆã‚Šã€ã‚¤ãƒ³ãƒˆãƒ­ã®é™ã‹ãªéƒ¨åˆ†ã‚„ã‚¢ã‚¦ãƒˆãƒ­ã‚’é¿ã‘ã€
    ã‚³ãƒ¼ãƒ‰é€²è¡ŒãŒæ˜ç¢ºãªã‚µãƒ“/ã‚³ãƒ¼ãƒ©ã‚¹éƒ¨åˆ†ã‚’å„ªå…ˆçš„ã«è§£æã§ãã‚‹ã€‚

    Args:
        y: éŸ³å£°æ³¢å½¢
        sr: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆ
        target_duration: åˆ‡ã‚Šå‡ºã™é•·ã•ï¼ˆç§’ï¼‰
        hop_length: RMSè¨ˆç®—ã®ãƒ›ãƒƒãƒ—é•·

    Returns:
        Tuple[ndarray, float, float]: (é¸æŠã•ã‚ŒãŸåŒºé–“ã®æ³¢å½¢, é–‹å§‹æ™‚åˆ», çµ‚äº†æ™‚åˆ»)
    """
    full_duration = len(y) / sr

    # çŸ­ã„éŸ³æºã®å ´åˆã¯ãã®ã¾ã¾è¿”ã™
    if full_duration <= target_duration:
        return y, 0.0, full_duration

    # RMSã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚’è¨ˆç®—
    rms = librosa.feature.rms(y=y, hop_length=hop_length)[0]

    # ç§»å‹•å¹³å‡ã§ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°ï¼ˆtarget_durationç›¸å½“ã®ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ï¼‰
    window_frames = int(target_duration * sr / hop_length)
    if window_frames >= len(rms):
        return y[:int(target_duration * sr)], 0.0, target_duration

    # ç•³ã¿è¾¼ã¿ã§ç§»å‹•å¹³å‡RMSã‚’è¨ˆç®—
    kernel = np.ones(window_frames) / window_frames
    smoothed_rms = np.convolve(rms, kernel, mode='valid')

    # æœ€å¤§RMSã®ä½ç½®ã‚’å–å¾—
    best_frame = np.argmax(smoothed_rms)
    best_start_time = best_frame * hop_length / sr
    best_end_time = best_start_time + target_duration

    # ç¯„å›²å¤–ãƒã‚§ãƒƒã‚¯
    if best_end_time > full_duration:
        best_end_time = full_duration
        best_start_time = max(0, full_duration - target_duration)

    # æ³¢å½¢ã‚’åˆ‡ã‚Šå‡ºã—
    start_sample = int(best_start_time * sr)
    end_sample = int(best_end_time * sr)

    return y[start_sample:end_sample], best_start_time, best_end_time


def estimate_key_enhanced(y_harmonic, sr):
    """
    æ”¹è‰¯ç‰ˆã‚­ãƒ¼æ¨å®šã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 

    ãƒãƒ¼ãƒ¢ãƒ‹ãƒƒã‚¯æˆåˆ†ã‹ã‚‰CQTãƒ™ãƒ¼ã‚¹ã®Chromaç‰¹å¾´é‡ã‚’è¨ˆç®—ã—ã€
    Krumhansl-Schmuckler key-finding algorithmã®æ”¹è‰¯ç‰ˆã§æ¨å®šã€‚

    æ”¹å–„ç‚¹ï¼š
    - CQTãƒ™ãƒ¼ã‚¹ã®Chromaï¼ˆå‘¨æ³¢æ•°è§£åƒåº¦ãŒé«˜ã„ï¼‰
    - CENSæ­£è¦åŒ–ï¼ˆã‚¨ãƒãƒ«ã‚®ãƒ¼æ­£è¦åŒ–ï¼‹ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°ï¼‰
    - è¤‡æ•°ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«æ¯”è¼ƒ

    Returns:
        Tuple[str, str, float]: (rootNote, scale, confidence)
    """
    # CQTãƒ™ãƒ¼ã‚¹ã®Chromaï¼ˆã‚ˆã‚Šç²¾åº¦ãŒé«˜ã„ï¼‰
    chroma_cqt = librosa.feature.chroma_cqt(y=y_harmonic, sr=sr, hop_length=512)

    # CENSï¼ˆChroma Energy Normalized Statisticsï¼‰ã§å®‰å®šåŒ–
    chroma_cens = librosa.feature.chroma_cens(y=y_harmonic, sr=sr, hop_length=512)

    # ä¸¡æ–¹ã‚’çµ„ã¿åˆã‚ã›ï¼ˆCQTã®ç²¾åº¦ + CENSã®å®‰å®šæ€§ï¼‰
    chroma_combined = (chroma_cqt + chroma_cens) / 2

    # æ™‚é–“è»¸ã§å¹³å‡åŒ–
    chroma_mean = np.mean(chroma_combined, axis=1)

    # æ­£è¦åŒ–
    chroma_mean = chroma_mean / (np.sum(chroma_mean) + 1e-8)

    # Krumhansl-Kessler ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
    # ãƒ¡ã‚¸ãƒ£ãƒ¼: çµŒé¨“çš„ã«èª¿æ•´ã•ã‚ŒãŸé‡ã¿
    major_profile = np.array([6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88])

    # ãƒã‚¤ãƒŠãƒ¼: çµŒé¨“çš„ã«èª¿æ•´ã•ã‚ŒãŸé‡ã¿
    minor_profile = np.array([6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17])

    # æ­£è¦åŒ–
    major_profile = major_profile / np.sum(major_profile)
    minor_profile = minor_profile / np.sum(minor_profile)

    note_names = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']

    best_score = -1.0
    best_key = 'C'
    best_scale = 'ãƒ¡ã‚¸ãƒ£ãƒ¼'

    for i in range(12):
        # ãƒ¡ã‚¸ãƒ£ãƒ¼
        shifted_major = np.roll(major_profile, i)
        major_corr = np.corrcoef(chroma_mean, shifted_major)[0, 1]

        if major_corr > best_score:
            best_score = major_corr
            best_key = note_names[i]
            best_scale = 'ãƒ¡ã‚¸ãƒ£ãƒ¼'

        # ãƒã‚¤ãƒŠãƒ¼
        shifted_minor = np.roll(minor_profile, i)
        minor_corr = np.corrcoef(chroma_mean, shifted_minor)[0, 1]

        if minor_corr > best_score:
            best_score = minor_corr
            best_key = note_names[i]
            best_scale = 'ãƒã‚¤ãƒŠãƒ¼'

    # ä¿¡é ¼åº¦ï¼ˆç›¸é–¢ä¿‚æ•°ã‚’0-1ã«ãƒãƒƒãƒ”ãƒ³ã‚°ï¼‰
    confidence = (best_score + 1.0) / 2.0
    confidence = max(0.0, min(1.0, confidence))

    return best_key, best_scale, confidence


def detect_chords_beat_synced(y_harmonic, sr, beats, key_root, key_scale, time_offset=0.0):
    """
    ãƒ“ãƒ¼ãƒˆåŒæœŸã‚³ãƒ¼ãƒ‰é€²è¡Œæ¤œå‡º

    ãƒ“ãƒ¼ãƒˆä½ç½®ã«åˆã‚ã›ã¦ã‚³ãƒ¼ãƒ‰ã‚’æ¤œå‡ºã™ã‚‹ã“ã¨ã§ã€
    å›ºå®šæ™‚é–“åŒºåˆ‡ã‚Šã‚ˆã‚Šè‡ªç„¶ãªã‚³ãƒ¼ãƒ‰é€²è¡Œã‚’å–å¾—ã€‚

    Args:
        y_harmonic: ãƒãƒ¼ãƒ¢ãƒ‹ãƒƒã‚¯æˆåˆ†
        sr: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆ
        beats: ãƒ“ãƒ¼ãƒˆä½ç½®ã®é…åˆ—ï¼ˆç§’ï¼‰
        key_root: ã‚­ãƒ¼ã®ãƒ«ãƒ¼ãƒˆéŸ³
        key_scale: ã‚¹ã‚±ãƒ¼ãƒ«å
        time_offset: å…ƒéŸ³æºã§ã®é–‹å§‹æ™‚åˆ»ã‚ªãƒ•ã‚»ãƒƒãƒˆ

    Returns:
        List[ChordInfo]: ã‚³ãƒ¼ãƒ‰é€²è¡Œ
    """
    chord_progression = []
    note_names = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']

    # 4æ‹ï¼ˆ1å°ç¯€ï¼‰ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
    bars = []
    for i in range(0, len(beats) - 3, 4):
        bar_start = beats[i]
        bar_end = beats[min(i + 4, len(beats) - 1)]
        bars.append((bar_start, bar_end))

    # æœ€å¾Œã®ç«¯æ•°ã‚‚è¿½åŠ 
    if len(beats) > 0 and len(bars) > 0:
        last_bar_end = bars[-1][1]
        if beats[-1] > last_bar_end:
            bars.append((last_bar_end, beats[-1]))

    for bar_start, bar_end in bars:
        start_sample = int(bar_start * sr)
        end_sample = int(bar_end * sr)
        segment = y_harmonic[start_sample:end_sample]

        if len(segment) < 1024:
            continue

        # CQTãƒ™ãƒ¼ã‚¹ã®Chromaã‚’è¨ˆç®—
        chroma = librosa.feature.chroma_cqt(y=segment, sr=sr, hop_length=256)
        chroma_mean = np.mean(chroma, axis=1)

        # ã‚³ãƒ¼ãƒ‰ã‚’æ¨å®š
        chord_name, root_note, quality, confidence = match_chord_enhanced(
            chroma_mean, note_names, key_root, key_scale
        )

        chord_progression.append(ChordInfo(
            startTime=bar_start + time_offset,
            endTime=bar_end + time_offset,
            chord=chord_name,
            rootNote=root_note,
            quality=quality,
            confidence=confidence
        ))

    # é€£ç¶šã™ã‚‹åŒã˜ã‚³ãƒ¼ãƒ‰ã‚’ãƒãƒ¼ã‚¸
    chord_progression = merge_consecutive_chords(chord_progression)

    return chord_progression


def detect_chords_enhanced(y_harmonic, sr, key_root, key_scale, time_offset=0.0):
    """
    æ”¹è‰¯ç‰ˆã‚³ãƒ¼ãƒ‰æ¤œå‡ºï¼ˆãƒ“ãƒ¼ãƒˆæƒ…å ±ãŒãªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰

    2ç§’åŒºåˆ‡ã‚Šã§ã‚ˆã‚Šç´°ã‹ãã‚³ãƒ¼ãƒ‰ã‚’æ¤œå‡ºã€‚

    Returns:
        List[ChordInfo]: ã‚³ãƒ¼ãƒ‰é€²è¡Œ
    """
    duration = len(y_harmonic) / sr
    segment_duration = 2.0  # 2ç§’ã”ã¨ï¼ˆå¾“æ¥ã®4ç§’ã‚ˆã‚Šç´°ã‹ãï¼‰
    num_segments = int(np.ceil(duration / segment_duration))

    chord_progression = []
    note_names = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']

    for seg_idx in range(num_segments):
        start_time = seg_idx * segment_duration
        end_time = min((seg_idx + 1) * segment_duration, duration)

        start_sample = int(start_time * sr)
        end_sample = int(end_time * sr)
        segment = y_harmonic[start_sample:end_sample]

        if len(segment) < 1024:
            continue

        # CQTãƒ™ãƒ¼ã‚¹ã®Chroma
        chroma = librosa.feature.chroma_cqt(y=segment, sr=sr, hop_length=256)
        chroma_mean = np.mean(chroma, axis=1)

        chord_name, root_note, quality, confidence = match_chord_enhanced(
            chroma_mean, note_names, key_root, key_scale
        )

        chord_progression.append(ChordInfo(
            startTime=start_time + time_offset,
            endTime=end_time + time_offset,
            chord=chord_name,
            rootNote=root_note,
            quality=quality,
            confidence=confidence
        ))

    # é€£ç¶šã™ã‚‹åŒã˜ã‚³ãƒ¼ãƒ‰ã‚’ãƒãƒ¼ã‚¸
    chord_progression = merge_consecutive_chords(chord_progression)

    return chord_progression


def match_chord_enhanced(chroma_values, note_names, key_root, key_scale):
    """
    æ”¹è‰¯ç‰ˆã‚³ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°

    ãƒ€ã‚¤ã‚¢ãƒˆãƒ‹ãƒƒã‚¯ã‚³ãƒ¼ãƒ‰ + ã‚ˆãä½¿ã‚ã‚Œã‚‹å€Ÿç”¨ã‚³ãƒ¼ãƒ‰ã‚‚å«ã‚ã¦ãƒãƒƒãƒãƒ³ã‚°ã€‚

    Returns:
        Tuple[str, str, str, float]: (chord_name, root_note, quality, confidence)
    """
    root_index = note_names.index(key_root)

    # ãƒ€ã‚¤ã‚¢ãƒˆãƒ‹ãƒƒã‚¯ã‚³ãƒ¼ãƒ‰ + å€Ÿç”¨ã‚³ãƒ¼ãƒ‰
    if key_scale == 'ãƒ¡ã‚¸ãƒ£ãƒ¼':
        chord_templates = [
            {'offset': 0, 'quality': 'maj', 'degree': 'I'},
            {'offset': 2, 'quality': 'min', 'degree': 'ii'},
            {'offset': 4, 'quality': 'min', 'degree': 'iii'},
            {'offset': 5, 'quality': 'maj', 'degree': 'IV'},
            {'offset': 7, 'quality': 'maj', 'degree': 'V'},
            {'offset': 9, 'quality': 'min', 'degree': 'vi'},
            {'offset': 11, 'quality': 'dim', 'degree': 'viiÂ°'},
            # å€Ÿç”¨ã‚³ãƒ¼ãƒ‰
            {'offset': 10, 'quality': 'maj', 'degree': 'bVII'},  # â™­VII
            {'offset': 8, 'quality': 'maj', 'degree': 'bVI'},   # â™­VI
        ]
    else:
        chord_templates = [
            {'offset': 0, 'quality': 'min', 'degree': 'i'},
            {'offset': 2, 'quality': 'dim', 'degree': 'iiÂ°'},
            {'offset': 3, 'quality': 'maj', 'degree': 'III'},
            {'offset': 5, 'quality': 'min', 'degree': 'iv'},
            {'offset': 7, 'quality': 'min', 'degree': 'v'},
            {'offset': 7, 'quality': 'maj', 'degree': 'V'},    # ãƒãƒ¼ãƒ¢ãƒ‹ãƒƒã‚¯ãƒã‚¤ãƒŠãƒ¼ã®V
            {'offset': 8, 'quality': 'maj', 'degree': 'VI'},
            {'offset': 10, 'quality': 'maj', 'degree': 'VII'},
        ]

    best_match_score = -1.0
    best_chord = chord_templates[0]

    # æ­£è¦åŒ–
    chroma_norm = chroma_values / (np.sum(chroma_values) + 1e-8)

    for chord_info in chord_templates:
        chord_root_index = (root_index + chord_info['offset']) % 12

        # ã‚³ãƒ¼ãƒ‰ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆï¼ˆãƒ«ãƒ¼ãƒˆã€3åº¦ã€5åº¦ï¼‰
        chord_template = np.zeros(12)
        chord_template[chord_root_index] = 1.0  # ãƒ«ãƒ¼ãƒˆ

        if chord_info['quality'] == 'maj':
            chord_template[(chord_root_index + 4) % 12] = 0.7  # é•·3åº¦
            chord_template[(chord_root_index + 7) % 12] = 0.5  # å®Œå…¨5åº¦
        elif chord_info['quality'] == 'min':
            chord_template[(chord_root_index + 3) % 12] = 0.7  # çŸ­3åº¦
            chord_template[(chord_root_index + 7) % 12] = 0.5  # å®Œå…¨5åº¦
        else:  # dim
            chord_template[(chord_root_index + 3) % 12] = 0.7  # çŸ­3åº¦
            chord_template[(chord_root_index + 6) % 12] = 0.5  # æ¸›5åº¦

        # æ­£è¦åŒ–
        chord_template = chord_template / (np.sum(chord_template) + 1e-8)

        # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã§ä¸€è‡´åº¦ã‚’è¨ˆç®—
        match_score = np.dot(chroma_norm, chord_template)

        if match_score > best_match_score:
            best_match_score = match_score
            best_chord = chord_info

    # ã‚³ãƒ¼ãƒ‰åç”Ÿæˆ
    chord_root_note = note_names[(root_index + best_chord['offset']) % 12]
    if best_chord['quality'] == 'maj':
        chord_name = chord_root_note
    elif best_chord['quality'] == 'min':
        chord_name = chord_root_note + 'm'
    else:
        chord_name = chord_root_note + 'dim'

    confidence = min(1.0, max(0.0, best_match_score))

    return chord_name, chord_root_note, best_chord['quality'], confidence


def merge_consecutive_chords(chord_progression: List[ChordInfo]) -> List[ChordInfo]:
    """
    é€£ç¶šã™ã‚‹åŒã˜ã‚³ãƒ¼ãƒ‰ã‚’ãƒãƒ¼ã‚¸

    ç´°ã‹ãæ¤œå‡ºã—ãŸã‚³ãƒ¼ãƒ‰ã®ä¸­ã§ã€é€£ç¶šã—ã¦åŒã˜ã‚³ãƒ¼ãƒ‰ãŒç¶šãå ´åˆã¯
    1ã¤ã«ã¾ã¨ã‚ã¦å¯èª­æ€§ã‚’å‘ä¸Šã€‚

    Returns:
        List[ChordInfo]: ãƒãƒ¼ã‚¸å¾Œã®ã‚³ãƒ¼ãƒ‰é€²è¡Œ
    """
    if not chord_progression:
        return []

    merged = []
    current = chord_progression[0]

    for next_chord in chord_progression[1:]:
        if next_chord.chord == current.chord:
            # åŒã˜ã‚³ãƒ¼ãƒ‰ãªã‚‰çµ‚äº†æ™‚åˆ»ã‚’å»¶é•·
            current = ChordInfo(
                startTime=current.startTime,
                endTime=next_chord.endTime,
                chord=current.chord,
                rootNote=current.rootNote,
                quality=current.quality,
                confidence=(current.confidence + next_chord.confidence) / 2
            )
        else:
            # é•ã†ã‚³ãƒ¼ãƒ‰ãªã‚‰ç¾åœ¨ã®ã‚³ãƒ¼ãƒ‰ã‚’ç¢ºå®šã—ã¦æ¬¡ã¸
            merged.append(current)
            current = next_chord

    # æœ€å¾Œã®ã‚³ãƒ¼ãƒ‰ã‚’è¿½åŠ 
    merged.append(current)

    return merged


def estimate_key_simple(y, sr):
    """
    ç°¡æ˜“ã‚­ãƒ¼æ¨å®šã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 

    chromaãƒ™ãƒ¼ã‚¹ã®12æ¬¡å…ƒãƒ”ãƒƒãƒã‚¯ãƒ©ã‚¹ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã—ã€
    ãƒ¡ã‚¸ãƒ£ãƒ¼/ãƒã‚¤ãƒŠãƒ¼ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã¨ç›¸é–¢ã‚’å–ã£ã¦æ¨å®šã™ã‚‹ã€‚

    Returns:
        Tuple[str, str, float]: (rootNote, scale, confidence)
    """

    # chromaç‰¹å¾´é‡ã‚’è¨ˆç®—ï¼ˆã‚¯ãƒ­ãƒã‚°ãƒ©ãƒ ï¼‰
    chroma = librosa.feature.chroma_stft(y=y, sr=sr)

    # æ™‚é–“è»¸ã§å¹³å‡åŒ–ã—ã¦12æ¬¡å…ƒã®ãƒ”ãƒƒãƒã‚¯ãƒ©ã‚¹ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã«
    chroma_mean = np.mean(chroma, axis=1)

    # æ­£è¦åŒ–
    chroma_mean = chroma_mean / (np.sum(chroma_mean) + 1e-8)

    # ãƒ¡ã‚¸ãƒ£ãƒ¼/ãƒã‚¤ãƒŠãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆï¼ˆKrumhansl-Kessler profilesï¼‰
    # ãƒ¡ã‚¸ãƒ£ãƒ¼: ãƒ‰(1.0), ãƒ¬(0.3), ãƒŸ(0.8), ãƒ•ã‚¡(0.4), ã‚½(0.9), ãƒ©(0.5), ã‚·(0.7)
    major_template = np.array([1.0, 0.2, 0.3, 0.2, 0.8, 0.4, 0.2, 0.9, 0.3, 0.5, 0.3, 0.7])

    # ãƒã‚¤ãƒŠãƒ¼: ãƒ©(1.0), ã‚·(0.3), ãƒ‰(0.8), ãƒ¬(0.4), ãƒŸ(0.9), ãƒ•ã‚¡(0.5), ã‚½(0.7)
    # ï¼ˆAãƒã‚¤ãƒŠãƒ¼ãƒ™ãƒ¼ã‚¹ â†’ 0ã‹ã‚‰å§‹ã¾ã‚‹ã‚ˆã†ã«9ã¤ã‚·ãƒ•ãƒˆï¼‰
    minor_template = np.array([1.0, 0.2, 0.8, 0.3, 0.4, 0.9, 0.2, 0.5, 0.2, 0.7, 0.3, 0.3])

    # æ­£è¦åŒ–
    major_template = major_template / np.sum(major_template)
    minor_template = minor_template / np.sum(minor_template)

    # 12éŸ³ã™ã¹ã¦ã®å¯èƒ½æ€§ã‚’è©¦ã™ï¼ˆç›¸é–¢ä¿‚æ•°ã§è©•ä¾¡ï¼‰
    note_names = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']

    best_score = -1.0
    best_key = 'C'
    best_scale = 'ãƒ¡ã‚¸ãƒ£ãƒ¼'

    for i in range(12):
        # ãƒ¡ã‚¸ãƒ£ãƒ¼ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’iåŠéŸ³åˆ†ã‚·ãƒ•ãƒˆ
        shifted_major = np.roll(major_template, i)
        major_corr = np.corrcoef(chroma_mean, shifted_major)[0, 1]

        if major_corr > best_score:
            best_score = major_corr
            best_key = note_names[i]
            best_scale = 'ãƒ¡ã‚¸ãƒ£ãƒ¼'

        # ãƒã‚¤ãƒŠãƒ¼ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’iåŠéŸ³åˆ†ã‚·ãƒ•ãƒˆ
        shifted_minor = np.roll(minor_template, i)
        minor_corr = np.corrcoef(chroma_mean, shifted_minor)[0, 1]

        if minor_corr > best_score:
            best_score = minor_corr
            best_key = note_names[i]
            best_scale = 'ãƒã‚¤ãƒŠãƒ¼'

    # ä¿¡é ¼åº¦ã‚’0-1ã«æ­£è¦åŒ–ï¼ˆç›¸é–¢ä¿‚æ•°ã¯-1ã€œ1ãªã®ã§ã€0.5ã€œ1.0ã«ãƒãƒƒãƒ”ãƒ³ã‚°ï¼‰
    confidence = (best_score + 1.0) / 2.0
    confidence = max(0.0, min(1.0, confidence))

    return best_key, best_scale, confidence


def detect_chords_simple(y, sr, key_root, key_scale):
    """
    ç°¡æ˜“ã‚³ãƒ¼ãƒ‰é€²è¡Œæ¤œå‡º

    æ›²ã‚’4ç§’ã”ã¨ã®åŒºé–“ã«åˆ†å‰²ã—ã€å„åŒºé–“ã®chromaã‹ã‚‰
    ãã®ã‚­ãƒ¼ã®ãƒ€ã‚¤ã‚¢ãƒˆãƒ‹ãƒƒã‚¯ã‚³ãƒ¼ãƒ‰ã®ã©ã‚Œã«è¿‘ã„ã‹ã‚’åˆ¤å®šã™ã‚‹ã€‚

    Returns:
        List[ChordInfo]: ã‚³ãƒ¼ãƒ‰é€²è¡Œ
    """

    duration = len(y) / sr
    segment_duration = 4.0  # 4ç§’ã”ã¨
    num_segments = int(np.ceil(duration / segment_duration))

    # ãƒ€ã‚¤ã‚¢ãƒˆãƒ‹ãƒƒã‚¯ã‚³ãƒ¼ãƒ‰ã®å®šç¾©ï¼ˆç°¡æ˜“ç‰ˆï¼‰
    # ãƒ¡ã‚¸ãƒ£ãƒ¼ã‚­ãƒ¼ã®å ´åˆ: I, ii, iii, IV, V, vi, viiÂ°
    # ãƒã‚¤ãƒŠãƒ¼ã‚­ãƒ¼ã®å ´åˆ: i, iiÂ°, III, iv, v, VI, VII

    note_names = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']
    root_index = note_names.index(key_root)

    if key_scale == 'ãƒ¡ã‚¸ãƒ£ãƒ¼':
        # I, IV, V (maj), ii, iii, vi (min)
        diatonic_chords = [
            {'offset': 0, 'quality': 'maj', 'degree': 'I'},    # I (ãƒˆãƒ‹ãƒƒã‚¯)
            {'offset': 2, 'quality': 'min', 'degree': 'ii'},   # ii
            {'offset': 4, 'quality': 'min', 'degree': 'iii'},  # iii
            {'offset': 5, 'quality': 'maj', 'degree': 'IV'},   # IV (ã‚µãƒ–ãƒ‰ãƒŸãƒŠãƒ³ãƒˆ)
            {'offset': 7, 'quality': 'maj', 'degree': 'V'},    # V (ãƒ‰ãƒŸãƒŠãƒ³ãƒˆ)
            {'offset': 9, 'quality': 'min', 'degree': 'vi'},   # vi (ç›¸å¯¾ãƒã‚¤ãƒŠãƒ¼)
        ]
    else:  # ãƒã‚¤ãƒŠãƒ¼
        # i, iv, v (min), III, VI, VII (maj)
        diatonic_chords = [
            {'offset': 0, 'quality': 'min', 'degree': 'i'},    # i (ãƒˆãƒ‹ãƒƒã‚¯)
            {'offset': 3, 'quality': 'maj', 'degree': 'III'},  # III
            {'offset': 5, 'quality': 'min', 'degree': 'iv'},   # iv
            {'offset': 7, 'quality': 'min', 'degree': 'v'},    # v
            {'offset': 8, 'quality': 'maj', 'degree': 'VI'},   # VI
            {'offset': 10, 'quality': 'maj', 'degree': 'VII'}, # VII
        ]

    chord_progression = []

    for seg_idx in range(num_segments):
        start_time = seg_idx * segment_duration
        end_time = min((seg_idx + 1) * segment_duration, duration)

        # ã“ã®åŒºé–“ã®éŸ³å£°ã‚’åˆ‡ã‚Šå‡ºã—
        start_sample = int(start_time * sr)
        end_sample = int(end_time * sr)
        y_segment = y[start_sample:end_sample]

        if len(y_segment) < sr * 0.5:  # 0.5ç§’æœªæº€ãªã‚‰ç„¡è¦–
            continue

        # chromaç‰¹å¾´é‡ã‚’è¨ˆç®—
        chroma_seg = librosa.feature.chroma_stft(y=y_segment, sr=sr)
        chroma_mean = np.mean(chroma_seg, axis=1)

        # å„ãƒ€ã‚¤ã‚¢ãƒˆãƒ‹ãƒƒã‚¯ã‚³ãƒ¼ãƒ‰ã¨ã®ä¸€è‡´åº¦ã‚’è¨ˆç®—
        best_match_score = -1.0
        best_chord = diatonic_chords[0]  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ãƒˆãƒ‹ãƒƒã‚¯

        for chord_info in diatonic_chords:
            chord_root_index = (root_index + chord_info['offset']) % 12

            # ã‚³ãƒ¼ãƒ‰ã®ãƒ«ãƒ¼ãƒˆéŸ³ã€ç¬¬3éŸ³ã€ç¬¬5éŸ³ã‚’å¼·èª¿ã—ãŸãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
            chord_template = np.zeros(12)
            chord_template[chord_root_index] = 1.0  # ãƒ«ãƒ¼ãƒˆ

            if chord_info['quality'] == 'maj':
                chord_template[(chord_root_index + 4) % 12] = 0.8  # é•·3åº¦
            else:  # min
                chord_template[(chord_root_index + 3) % 12] = 0.8  # çŸ­3åº¦

            chord_template[(chord_root_index + 7) % 12] = 0.6  # å®Œå…¨5åº¦

            # æ­£è¦åŒ–
            chord_template = chord_template / (np.sum(chord_template) + 1e-8)
            chroma_mean_norm = chroma_mean / (np.sum(chroma_mean) + 1e-8)

            # å†…ç©ã§ä¸€è‡´åº¦ã‚’è¨ˆç®—
            match_score = np.dot(chroma_mean_norm, chord_template)

            if match_score > best_match_score:
                best_match_score = match_score
                best_chord = chord_info

        # ã‚³ãƒ¼ãƒ‰åã‚’ç”Ÿæˆ
        chord_root_note = note_names[(root_index + best_chord['offset']) % 12]

        if best_chord['quality'] == 'maj':
            chord_name = chord_root_note
        else:
            chord_name = chord_root_note + 'm'

        # ä¿¡é ¼åº¦ã‚’0-1ã«ãƒãƒƒãƒ”ãƒ³ã‚°
        confidence = min(1.0, max(0.0, best_match_score))

        chord_progression.append(ChordInfo(
            startTime=start_time,
            endTime=end_time,
            chord=chord_name,
            rootNote=chord_root_note,
            quality=best_chord['quality'],
            confidence=confidence
        ))

    return chord_progression


def generate_fallback_chords(key_root: str, duration: float):
    """
    ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚³ãƒ¼ãƒ‰é€²è¡Œã‚’ç”Ÿæˆ
    """
    return [
        ChordInfo(
            startTime=0.0, endTime=4.0,
            chord=key_root, rootNote=key_root, quality='maj', confidence=0.5
        ),
    ]


def generate_scale_match(detected_key: str, scale_name: str, chord_progression: List[ChordInfo]):
    """
    ã‚¹ã‚±ãƒ¼ãƒ«ãƒãƒƒãƒãƒ³ã‚°çµæœã‚’ç”Ÿæˆ

    1ä½: æ¨å®šã•ã‚ŒãŸã‚­ãƒ¼
    2ä½: ç›¸å¯¾èª¿ï¼ˆãƒ¡ã‚¸ãƒ£ãƒ¼â†”ãƒã‚¤ãƒŠãƒ¼ï¼‰
    """

    # æ¤œå‡ºã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰ã®ãƒªã‚¹ãƒˆ
    detected_chords = list(set([c.chord for c in chord_progression]))

    # 1ä½: æ¨å®šã‚­ãƒ¼
    first_match = ScaleMatchInfo(
        scale=scale_name,
        rootNote=detected_key,
        matchRate=0.92,
        matchingChords=detected_chords
    )

    # 2ä½: ç›¸å¯¾èª¿ã‚’è¨ˆç®—
    note_names = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']
    root_index = note_names.index(detected_key)

    if scale_name == 'ãƒ¡ã‚¸ãƒ£ãƒ¼':
        # ç›¸å¯¾ãƒã‚¤ãƒŠãƒ¼ï¼ˆ6åº¦ä¸‹ = çŸ­6åº¦ä¸Š = 9åŠéŸ³ä¸Š = 3åŠéŸ³ä¸‹ï¼‰
        relative_index = (root_index - 3) % 12
        relative_scale = 'ãƒã‚¤ãƒŠãƒ¼'
    else:
        # ç›¸å¯¾ãƒ¡ã‚¸ãƒ£ãƒ¼ï¼ˆ3åŠéŸ³ä¸Šï¼‰
        relative_index = (root_index + 3) % 12
        relative_scale = 'ãƒ¡ã‚¸ãƒ£ãƒ¼'

    relative_key = note_names[relative_index]

    second_match = ScaleMatchInfo(
        scale=relative_scale,
        rootNote=relative_key,
        matchRate=0.85,
        matchingChords=detected_chords
    )

    # 3ä½: ãƒŸã‚¯ã‚½ãƒªãƒ‡ã‚£ã‚¢ãƒ³ï¼ˆãŠã¾ã‘ï¼‰
    third_match = ScaleMatchInfo(
        scale='ãƒŸã‚¯ã‚½ãƒªãƒ‡ã‚£ã‚¢ãƒ³',
        rootNote=detected_key,
        matchRate=0.75,
        matchingChords=detected_chords[:3] if len(detected_chords) >= 3 else detected_chords
    )

    return ScaleMatchResult(
        matchingScales=[first_match, second_match, third_match]
    )


# ============================================
# Essentia ãƒ™ãƒ¼ã‚¹è§£æãƒ­ã‚¸ãƒƒã‚¯ï¼ˆé«˜ç²¾åº¦ãƒ¢ãƒ¼ãƒ‰ï¼‰
# ============================================

def analyze_audio_essentia(file_path: str, options: Dict) -> AnalysisResult:
    """
    Essentiaãƒ™ãƒ¼ã‚¹ã®é«˜ç²¾åº¦éŸ³æºè§£æ

    ç‰¹å¾´:
    - KeyExtractor (bgateãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«) ã§ã‚­ãƒ¼æ¤œå‡ºç²¾åº¦90%+
    - RhythmExtractor2013 ã§ãƒ“ãƒ¼ãƒˆæ¤œå‡º
    - ChordsDetectionBeats ã§ãƒ“ãƒ¼ãƒˆåŒæœŸã‚³ãƒ¼ãƒ‰æ¤œå‡º

    Args:
        file_path: éŸ³æºãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        options: è§£æã‚ªãƒ—ã‚·ãƒ§ãƒ³

    Returns:
        AnalysisResult: è§£æçµæœ
    """

    print(f"\n[Essentia Analysis] Starting high-precision analysis...")
    print(f"  File: {file_path}")

    # 1. éŸ³å£°èª­ã¿è¾¼ã¿ï¼ˆMonoLoader: 44100Hz, ãƒ¢ãƒãƒ©ãƒ«ï¼‰
    try:
        print(f"  Step 1/5: Loading audio file...")
        loader = es.MonoLoader(filename=file_path, sampleRate=44100)
        audio = loader()
        duration = len(audio) / 44100.0
        print(f"    âœ“ Loaded: {duration:.2f}s, sr=44100Hz, samples={len(audio)}")

        # 60ç§’ã«åˆ¶é™ï¼ˆé•·ã„æ›²ã®å‡¦ç†æ™‚é–“çŸ­ç¸®ï¼‰
        if duration > 60.0:
            audio = audio[:int(60.0 * 44100)]
            duration = 60.0
            print(f"    â†’ Truncated to 60.0s for processing efficiency")
    except Exception as e:
        print(f"    âœ— Failed to load audio file: {str(e)}")
        raise Exception(f"Failed to load audio file: {str(e)}")

    # 2. ã‚­ãƒ¼æ¤œå‡ºï¼ˆKeyExtractor + bgateãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰
    try:
        print(f"  Step 2/5: Detecting key with KeyExtractor (bgate profile)...")
        detected_key, scale_name, confidence = estimate_key_essentia(audio)
        print(f"    âœ“ Key detected: {detected_key} {scale_name} (confidence: {confidence:.2f})")
    except Exception as e:
        print(f"    âš  Key detection failed: {e}, using librosa fallback")
        # librosaãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        y, sr = librosa.load(file_path, sr=None, mono=True, duration=60.0)
        detected_key, scale_name, confidence = estimate_key_simple(y, sr)

    # 3. ãƒ†ãƒ³ãƒãƒ»ãƒ“ãƒ¼ãƒˆæ¤œå‡ºï¼ˆRhythmExtractor2013ï¼‰
    try:
        print(f"  Step 3/5: Detecting tempo and beats...")
        tempo, beats = detect_tempo_essentia(audio)
        print(f"    âœ“ Tempo detected: {tempo:.1f} BPM, {len(beats)} beats")
    except Exception as e:
        print(f"    âš  Tempo detection failed: {e}, using default 120 BPM")
        tempo = 120.0
        beats = []

    # 4. ã‚³ãƒ¼ãƒ‰é€²è¡Œæ¤œå‡ºï¼ˆãƒ“ãƒ¼ãƒˆåŒæœŸï¼‰
    try:
        print(f"  Step 4/5: Detecting chord progression...")
        if len(beats) >= 2:
            chord_progression = detect_chords_essentia(audio, beats, detected_key, scale_name)
        else:
            # ãƒ“ãƒ¼ãƒˆãŒå°‘ãªã„å ´åˆã¯4ç§’åŒºåˆ‡ã‚Šãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            chord_progression = detect_chords_essentia_simple(audio, duration, detected_key, scale_name)
        print(f"    âœ“ Detected {len(chord_progression)} chord segments")
        if len(chord_progression) > 0:
            print(f"    First chord: {chord_progression[0].chord} ({chord_progression[0].startTime:.1f}s - {chord_progression[0].endTime:.1f}s)")
    except Exception as e:
        print(f"    âš  Chord detection failed: {e}, using fallback")
        chord_progression = generate_fallback_chords(detected_key, duration)

    # 5. ã‚¹ã‚±ãƒ¼ãƒ«ãƒãƒƒãƒãƒ³ã‚°
    try:
        print(f"  Step 5/5: Generating scale matches...")
        scale_match = generate_scale_match(detected_key, scale_name, chord_progression)
        print(f"    âœ“ Generated {len(scale_match.matchingScales)} scale matches")
    except Exception as e:
        print(f"    âš  Scale matching failed: {e}")
        scale_match = ScaleMatchResult(matchingScales=[])

    # 6. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®æ§‹ç¯‰
    metadata = AnalysisMetadata(
        duration=duration,
        tempo=tempo,
        timeSignature="4/4",
        detectedKey=detected_key,
        scale=scale_name,
        confidence=confidence
    )

    print(f"\n[Essentia Analysis] Analysis completed successfully!")
    print(f"  Result: {detected_key} {scale_name}, {tempo:.1f} BPM, {len(chord_progression)} chords")
    print("=" * 60)

    return AnalysisResult(
        metadata=metadata,
        chordProgression=chord_progression,
        scaleMatch=scale_match,
        stems=None
    )


def estimate_key_essentia(audio) -> tuple:
    """
    Essentiaãƒ™ãƒ¼ã‚¹ã®ã‚­ãƒ¼æ¨å®š

    KeyExtractorã‚’ä½¿ç”¨ã—ã€bgateãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã§é«˜ç²¾åº¦ãªã‚­ãƒ¼æ¤œå‡ºã‚’è¡Œã†ã€‚
    bgateãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã¯é›»å­éŸ³æ¥½ãƒ»ãƒãƒƒãƒ—ã‚¹ã«æœ€é©åŒ–ã•ã‚Œã¦ã„ã‚‹ã€‚

    Returns:
        Tuple[str, str, float]: (rootNote, scale, confidence)
    """

    # KeyExtractorã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼ˆbgateãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰
    key_extractor = es.KeyExtractor(profileType='bgate')
    key, scale, strength = key_extractor(audio)

    # Essentiaã®ã‚¹ã‚±ãƒ¼ãƒ«åã‚’æ—¥æœ¬èªã«å¤‰æ›
    scale_name = 'ãƒ¡ã‚¸ãƒ£ãƒ¼' if scale == 'major' else 'ãƒã‚¤ãƒŠãƒ¼'

    # ä¿¡é ¼åº¦ã‚’0-1ã«ãƒãƒƒãƒ”ãƒ³ã‚°
    confidence = min(1.0, max(0.0, strength))

    return key, scale_name, confidence


def detect_tempo_essentia(audio) -> tuple:
    """
    Essentiaãƒ™ãƒ¼ã‚¹ã®ãƒ†ãƒ³ãƒãƒ»ãƒ“ãƒ¼ãƒˆæ¤œå‡º

    RhythmExtractor2013ã‚’ä½¿ç”¨ã—ã¦é«˜ç²¾åº¦ãªãƒ“ãƒ¼ãƒˆæ¤œå‡ºã‚’è¡Œã†ã€‚

    Returns:
        Tuple[float, List[float]]: (tempo, beats)
    """

    rhythm_extractor = es.RhythmExtractor2013(method="multifeature")
    bpm, beats, beats_confidence, _, beats_intervals = rhythm_extractor(audio)

    return float(bpm), list(beats)


def detect_chords_essentia(audio, beats: List[float], key_root: str, key_scale: str) -> List[ChordInfo]:
    """
    Essentiaãƒ™ãƒ¼ã‚¹ã®ãƒ“ãƒ¼ãƒˆåŒæœŸã‚³ãƒ¼ãƒ‰æ¤œå‡º

    ãƒ“ãƒ¼ãƒˆåŒºé–“ã”ã¨ã«HPCP (Harmonic Pitch Class Profile) ã‚’è¨ˆç®—ã—ã€
    ã‚³ãƒ¼ãƒ‰ã‚’æ¨å®šã™ã‚‹ã€‚

    Args:
        audio: éŸ³å£°ãƒ‡ãƒ¼ã‚¿
        beats: ãƒ“ãƒ¼ãƒˆä½ç½®ã®ãƒªã‚¹ãƒˆ
        key_root: ã‚­ãƒ¼ã®ãƒ«ãƒ¼ãƒˆéŸ³
        key_scale: ã‚¹ã‚±ãƒ¼ãƒ«å

    Returns:
        List[ChordInfo]: ã‚³ãƒ¼ãƒ‰é€²è¡Œ
    """

    sample_rate = 44100
    chord_progression = []

    # HPCPè¨ˆç®—ç”¨ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
    spectrum = es.Spectrum()
    spectral_peaks = es.SpectralPeaks(
        orderBy="magnitude",
        magnitudeThreshold=0.00001,
        minFrequency=20,
        maxFrequency=3500,
        maxPeaks=60
    )
    hpcp = es.HPCP(
        size=12,
        referenceFrequency=440,
        harmonics=8,
        bandPreset=True,
        minFrequency=20,
        maxFrequency=3500,
        weightType="cosine",
        nonLinear=False,
        windowSize=1.0
    )

    # ã‚³ãƒ¼ãƒ‰ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆï¼ˆãƒ¡ã‚¸ãƒ£ãƒ¼ãƒ»ãƒã‚¤ãƒŠãƒ¼ï¼‰
    note_names = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']

    # ãƒ“ãƒ¼ãƒˆã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ï¼ˆ2-4ãƒ“ãƒ¼ãƒˆã”ã¨ï¼‰
    beat_groups = []
    group_size = 4  # 4ãƒ“ãƒ¼ãƒˆã”ã¨ã«ã‚³ãƒ¼ãƒ‰æ¤œå‡º
    for i in range(0, len(beats) - 1, group_size):
        start_beat = beats[i]
        end_idx = min(i + group_size, len(beats) - 1)
        end_beat = beats[end_idx] if end_idx < len(beats) else beats[-1]
        beat_groups.append((start_beat, end_beat))

    for start_time, end_time in beat_groups:
        # ã“ã®åŒºé–“ã®éŸ³å£°ã‚’åˆ‡ã‚Šå‡ºã—
        start_sample = int(start_time * sample_rate)
        end_sample = int(end_time * sample_rate)
        segment = audio[start_sample:end_sample]

        if len(segment) < 2048:
            continue

        # ãƒ•ãƒ¬ãƒ¼ãƒ ã”ã¨ã«HPCPã‚’è¨ˆç®—ã—ã¦å¹³å‡
        frame_size = 2048
        hop_size = 1024
        hpcp_values = []

        for frame_start in range(0, len(segment) - frame_size, hop_size):
            frame = segment[frame_start:frame_start + frame_size]
            windowed = frame * np.hanning(len(frame))
            spec = spectrum(windowed)
            frequencies, magnitudes = spectral_peaks(spec)
            if len(frequencies) > 0:
                hpcp_frame = hpcp(frequencies, magnitudes)
                hpcp_values.append(hpcp_frame)

        if not hpcp_values:
            continue

        # HPCPã®å¹³å‡ã‚’è¨ˆç®—
        hpcp_mean = np.mean(hpcp_values, axis=0)

        # ã‚³ãƒ¼ãƒ‰ã‚’æ¨å®š
        chord_name, root_note, quality, confidence = match_chord_from_hpcp(
            hpcp_mean, note_names, key_root, key_scale
        )

        chord_progression.append(ChordInfo(
            startTime=start_time,
            endTime=end_time,
            chord=chord_name,
            rootNote=root_note,
            quality=quality,
            confidence=confidence
        ))

    return chord_progression


def detect_chords_essentia_simple(audio, duration: float, key_root: str, key_scale: str) -> List[ChordInfo]:
    """
    Essentiaãƒ™ãƒ¼ã‚¹ã®ç°¡æ˜“ã‚³ãƒ¼ãƒ‰æ¤œå‡ºï¼ˆ4ç§’åŒºåˆ‡ã‚Šï¼‰

    ãƒ“ãƒ¼ãƒˆæƒ…å ±ãŒãªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã€‚

    Returns:
        List[ChordInfo]: ã‚³ãƒ¼ãƒ‰é€²è¡Œ
    """

    sample_rate = 44100
    segment_duration = 4.0
    num_segments = int(np.ceil(duration / segment_duration))

    chord_progression = []
    note_names = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']

    # HPCPè¨ˆç®—ç”¨
    spectrum = es.Spectrum()
    spectral_peaks = es.SpectralPeaks(
        orderBy="magnitude",
        magnitudeThreshold=0.00001,
        minFrequency=20,
        maxFrequency=3500,
        maxPeaks=60
    )
    hpcp = es.HPCP(
        size=12,
        referenceFrequency=440,
        harmonics=8,
        bandPreset=True,
        minFrequency=20,
        maxFrequency=3500
    )

    for seg_idx in range(num_segments):
        start_time = seg_idx * segment_duration
        end_time = min((seg_idx + 1) * segment_duration, duration)

        start_sample = int(start_time * sample_rate)
        end_sample = int(end_time * sample_rate)
        segment = audio[start_sample:end_sample]

        if len(segment) < 2048:
            continue

        # ãƒ•ãƒ¬ãƒ¼ãƒ ã”ã¨ã«HPCPã‚’è¨ˆç®—
        frame_size = 2048
        hop_size = 1024
        hpcp_values = []

        for frame_start in range(0, len(segment) - frame_size, hop_size):
            frame = segment[frame_start:frame_start + frame_size]
            windowed = frame * np.hanning(len(frame))
            spec = spectrum(windowed)
            frequencies, magnitudes = spectral_peaks(spec)
            if len(frequencies) > 0:
                hpcp_frame = hpcp(frequencies, magnitudes)
                hpcp_values.append(hpcp_frame)

        if not hpcp_values:
            continue

        hpcp_mean = np.mean(hpcp_values, axis=0)
        chord_name, root_note, quality, confidence = match_chord_from_hpcp(
            hpcp_mean, note_names, key_root, key_scale
        )

        chord_progression.append(ChordInfo(
            startTime=start_time,
            endTime=end_time,
            chord=chord_name,
            rootNote=root_note,
            quality=quality,
            confidence=confidence
        ))

    return chord_progression


def match_chord_from_hpcp(hpcp_values, note_names: List[str], key_root: str, key_scale: str) -> tuple:
    """
    HPCPã‹ã‚‰ã‚³ãƒ¼ãƒ‰ã‚’æ¨å®š

    ãƒ€ã‚¤ã‚¢ãƒˆãƒ‹ãƒƒã‚¯ã‚³ãƒ¼ãƒ‰ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã¨ã®ä¸€è‡´åº¦ã§åˆ¤å®šã€‚

    Returns:
        Tuple[str, str, str, float]: (chord_name, root_note, quality, confidence)
    """

    root_index = note_names.index(key_root)

    # ãƒ€ã‚¤ã‚¢ãƒˆãƒ‹ãƒƒã‚¯ã‚³ãƒ¼ãƒ‰å®šç¾©
    if key_scale == 'ãƒ¡ã‚¸ãƒ£ãƒ¼':
        diatonic_chords = [
            {'offset': 0, 'quality': 'maj', 'degree': 'I'},
            {'offset': 2, 'quality': 'min', 'degree': 'ii'},
            {'offset': 4, 'quality': 'min', 'degree': 'iii'},
            {'offset': 5, 'quality': 'maj', 'degree': 'IV'},
            {'offset': 7, 'quality': 'maj', 'degree': 'V'},
            {'offset': 9, 'quality': 'min', 'degree': 'vi'},
        ]
    else:
        diatonic_chords = [
            {'offset': 0, 'quality': 'min', 'degree': 'i'},
            {'offset': 3, 'quality': 'maj', 'degree': 'III'},
            {'offset': 5, 'quality': 'min', 'degree': 'iv'},
            {'offset': 7, 'quality': 'min', 'degree': 'v'},
            {'offset': 8, 'quality': 'maj', 'degree': 'VI'},
            {'offset': 10, 'quality': 'maj', 'degree': 'VII'},
        ]

    best_match_score = -1.0
    best_chord = diatonic_chords[0]

    for chord_info in diatonic_chords:
        chord_root_index = (root_index + chord_info['offset']) % 12

        # ã‚³ãƒ¼ãƒ‰ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆï¼ˆãƒ«ãƒ¼ãƒˆã€3åº¦ã€5åº¦ï¼‰
        chord_template = np.zeros(12)
        chord_template[chord_root_index] = 1.0

        if chord_info['quality'] == 'maj':
            chord_template[(chord_root_index + 4) % 12] = 0.8  # é•·3åº¦
        else:
            chord_template[(chord_root_index + 3) % 12] = 0.8  # çŸ­3åº¦

        chord_template[(chord_root_index + 7) % 12] = 0.6  # å®Œå…¨5åº¦

        # æ­£è¦åŒ–
        chord_template = chord_template / (np.sum(chord_template) + 1e-8)
        hpcp_norm = hpcp_values / (np.sum(hpcp_values) + 1e-8)

        # å†…ç©ã§ä¸€è‡´åº¦
        match_score = np.dot(hpcp_norm, chord_template)

        if match_score > best_match_score:
            best_match_score = match_score
            best_chord = chord_info

    # ã‚³ãƒ¼ãƒ‰åç”Ÿæˆ
    chord_root_note = note_names[(root_index + best_chord['offset']) % 12]
    if best_chord['quality'] == 'maj':
        chord_name = chord_root_note
    else:
        chord_name = chord_root_note + 'm'

    confidence = min(1.0, max(0.0, best_match_score))

    return chord_name, chord_root_note, best_chord['quality'], confidence


# ============================================
# ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œç”¨ï¼ˆé–‹ç™ºæ™‚ã®ã¿ï¼‰
# ============================================

if __name__ == "__main__":
    import uvicorn

    # ãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºç”¨ã®èµ·å‹•ã‚³ãƒãƒ³ãƒ‰ï¼š
    # python main.py
    # ã¾ãŸã¯
    # uvicorn main:app --reload --host 0.0.0.0 --port 8000

    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        reload=True,  # ã‚³ãƒ¼ãƒ‰å¤‰æ›´æ™‚ã«è‡ªå‹•ãƒªãƒ­ãƒ¼ãƒ‰
        log_level="info"
    )

"""
ã€Phase 4 ä»¥é™ã§å®Ÿè£…äºˆå®šã®é–¢æ•°ã€‘

def separate_stems(file_path: str) -> Dict[str, str]:
    \"\"\"
    stem åˆ†è§£ï¼ˆDemucs ä½¿ç”¨ï¼‰

    Args:
        file_path: éŸ³æºãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹

    Returns:
        Dict[str, str]: stemåˆ¥ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            - vocals: ãƒœãƒ¼ã‚«ãƒ«ãƒˆãƒ©ãƒƒã‚¯
            - drums: ãƒ‰ãƒ©ãƒ ãƒˆãƒ©ãƒƒã‚¯
            - bass: ãƒ™ãƒ¼ã‚¹ãƒˆãƒ©ãƒƒã‚¯
            - other: ãã®ä»–ãƒˆãƒ©ãƒƒã‚¯
    \"\"\"
    # TODO: Demucs ã‚’ä½¿ã£ãŸå®Ÿè£…
    pass


def detect_chords(y: np.ndarray, sr: int) -> List[ChordInfo]:
    \"\"\"
    ã‚³ãƒ¼ãƒ‰é€²è¡Œæ¤œå‡ºï¼ˆmadmom or librosa ä½¿ç”¨ï¼‰

    Args:
        y: éŸ³æºãƒ‡ãƒ¼ã‚¿
        sr: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆ

    Returns:
        List[ChordInfo]: ã‚³ãƒ¼ãƒ‰é€²è¡Œ
    \"\"\"
    # TODO: madmom or librosa ã‚’ä½¿ã£ãŸå®Ÿè£…
    pass


def estimate_key(chroma: np.ndarray) -> Tuple[str, str]:
    \"\"\"
    ã‚­ãƒ¼æ¤œå‡º

    Args:
        chroma: ã‚¯ãƒ­ãƒã‚°ãƒ©ãƒ 

    Returns:
        Tuple[str, str]: (ã‚­ãƒ¼, ã‚¹ã‚±ãƒ¼ãƒ«)
    \"\"\"
    # TODO: ã‚­ãƒ¼æ¤œå‡ºã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®å®Ÿè£…
    pass
"""
